---
tags:
  - MIT
  - System
  - LLM
  - THU
  - Associate Professor

---

# Han Song

<div style="display: flex; justify-content: center;">
  <img src="https://assets-global.website-files.com/64f4d663be17b6544a586bac/6514db3bc943fa1c599d89d5_songhan.jpeg" alt="" width="200"/>
</div>

<p align="left">
  <a href="https://github.com/mit-han-lab">
    <img src="https://img.shields.io/badge/Github-white?logo=github&logoColor=black&cacheSeconds=1" alt="Github Badge"/>
  </a>
  <a href="https://twitter.com/songhan_mit">
    <img src="https://img.shields.io/badge/Twitter-white?logo=twitter&logoColor=blue&cacheSeconds=1" alt="Twitter Badge"/>
  </a>
  <a href="https://scholar.google.com/citations?user=E0iCaa4AAAAJ&hl=en">
    <img src="https://img.shields.io/badge/GoogleScholar-white?logo=googlescholar&logoColor=blue&cacheSeconds=1" alt="Google Scholar Badge"/>
  </a>
  <a href="mailto:hbh001098hbh@sjtu.edu.cn">
    <img src="https://img.shields.io/badge/Email-white?logo=gmail&logoColor=blue" alt="Email Badge"/>
  </a>
  <a href="https://hanlab.mit.edu/songhan">
  <img src="https://img.shields.io/badge/website-white?logo=wordpress&logoColor=blue" alt="Website Badge"/>
  </a>
</p>




## Biography

Song Han is an associate professor at MIT EECS. He received his PhD degree from Stanford University. He proposed the “Deep Compression” technique including pruning and quantization that is widely used for efficient AI computing, and “Efficient Inference Engine” that first brought weight sparsity to modern AI chips, which is a top-5 cited paper in 50 years of ISCA. He pioneered the TinyML research that brings deep learning to IoT devices, enabling learning on the edge (appeared on MIT home page). His team’s work on hardware-aware neural architecture search (once-for-all network) enables users to design, optimize, shrink and deploy AI models to resource-constrained hardware devices, receiving the first place in many low-power computer vision contests in flagship AI conferences.  His team’s recent work on large language model quantization/acceleration (SmoothQuant, AWQ, StreamingLLM) has effectively improved the efficiency of LLM inference, adopted by NVIDIA TensorRT-LLM. Song received best paper awards at ICLR and FPGA, faculty awards from Amazon, Facebook, NVIDIA, Samsung and SONY. Song was named “35 Innovators Under 35” by MIT Technology Review for his contribution on “deep compression” technique that “lets powerful artificial intelligence (AI) programs run more efficiently on low-power mobile devices.” Song received the NSF CAREER Award for “efficient algorithms and hardware for accelerated machine learning”, IEEE “AIs 10 to Watch: The Future of AI” award, and Sloan Research Fellowship. Song’s research in efficient AI computing has witnessed successful commercialization and influenced the industry. He was the cofounder of DeePhi (now part of AMD), and cofounder of OmniML (now part of NVIDIA). Song developed the EfficientML.ai course to disseminate efficient ML research.



Course：https://hanlab.mit.edu/courses/2023-fall-65940

## Selected PhDs

* [Haotian Tang](http://kentang.net/): graduated from SJTU IEEE Honor Class, 2020
* Zhijian Liu: 



## Selected Masters







## Recruitment Information

If you work on efficient LLM, VLM, GenAI and are interested in joining us, please fill in the [**recruiting form**](https://docs.google.com/forms/d/e/1FAIpQLSezX1BQnYyRoi_cmRCT2AGvnjetd4nJvxchoC2u4YLyK63qAg/viewform). Inquiry emails will not be replied if the recruiting form is incomplete. PhD applicants: select "ML+System" track in the MIT PhD application system.

